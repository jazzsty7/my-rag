import os
import re
import tempfile
from typing import List

import streamlit as st
from dotenv import load_dotenv

load_dotenv()

from langchain_community.document_loaders import PyPDFLoader
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.schema import Document
from langchain.callbacks.base import BaseCallbackHandler
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain import hub

# =================================================
# ê¸°ë³¸ UI
# =================================================
st.set_page_config(layout="wide")
st.title("ğŸ“˜ ìë™ì°¨ë³´í—˜ ì•½ê´€ RAG")

# =================================================
# ì„¸ì…˜ ìƒíƒœ (ë²„íŠ¼ í† ê¸€ ë°©ì§€)
# =================================================
if "show_answer" not in st.session_state:
    st.session_state.show_answer = False
if "show_original" not in st.session_state:
    st.session_state.show_original = False

# =================================================
# ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬
# =================================================
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.container = container
        self.text = ""

    def on_llm_new_token(self, token: str, **kwargs):
        self.text += token
        self.container.markdown(self.text)

# =================================================
# PDF ë¡œë“œ
# =================================================
def load_pdf(uploaded_file):
    tmp = tempfile.TemporaryDirectory()
    path = os.path.join(tmp.name, uploaded_file.name)
    with open(path, "wb") as f:
        f.write(uploaded_file.getvalue())
    return PyPDFLoader(path).load()

# =================================================
# ì •ê·œì‹
# =================================================
ARTICLE_START_RE = re.compile(r"(ì œ\s*\d+\s*ì¡°\s*\([^)]+\))")
STOP_RE = re.compile(r"(ì œ\s*\d+í¸|ì œ\s*\d+ì¥|ì œ\s*\d+ì ˆ)")

# =================================================
# ëª©ì°¨ íŒŒì‹± (í˜ì´ì§€ ì •í™•í™”)
# =================================================
def parse_toc(pages: List[Document]) -> dict:
    toc = {}
    for page in pages[:5]:
        for line in page.page_content.splitlines():
            # st.write(f"#### line: {line}")
            m = re.search(r"(ì œ\s*\d+ì¡°).*?(\d+)$", line.strip())
            if m:
                toc[m.group(1).replace(" ", "")] = int(m.group(2))
    return toc

# # =================================================
# # ì¡°í•­ íŒŒì‹±
# # =================================================
# def parse_articles(pages: List[Document]) -> List[Document]:
#     docs = []
#     buffer = ""
#     current_article = None
#     start_page = None

#     for page in pages:
#         page_no = page.metadata.get("page", 0) + 1
#         for line in page.page_content.splitlines():
#             # ì¡°í•­ ì‹œì‘ ê°ì§€
#             if ARTICLE_START_RE.match(line):
#                 # ì´ì „ ì¡°í•­ ì €ì¥
#                 if current_article:
#                     #st.write(f"ARTICLE_START_RE.Article: {current_article}, page_content: {buffer.strip()}")
#                     docs.append(Document(
#                         page_content=buffer.strip(),
#                         metadata={
#                             "article": current_article,
#                             "start_page": start_page,
#                             #"end_page": page_no - 1  # ì´ì „ í˜ì´ì§€ê¹Œì§€ ë²”ìœ„
#                             "end_page": page_no  # ì´ì „ í˜ì´ì§€ê¹Œì§€ ë²”ìœ„
#                         }
#                     ))
#                 current_article = line.strip()
#                 start_page = page_no
#                 buffer = line
#                 continue

#             # STOP_RE ê°ì§€: ì¡°í•­ ì¢…ë£Œ
#             if current_article and STOP_RE.match(line):
#                 st.write(f"STOP_RE.Article: {current_article}, line: {line.strip()}")
#                 docs.append(Document(
#                     page_content=buffer.strip(),
#                     metadata={
#                         "article": current_article,
#                         "start_page": start_page,
#                         "end_page": page_no
#                     }
#                 ))
#                 current_article = None
#                 start_page = None
#                 buffer = ""
#                 continue

#             # st.write(f"####### 222222222 line: {line.strip()}")

#             # ì¡°í•­ ë‚´ìš© ëˆ„ì 
#             if current_article:
#                 buffer += "\n" + line

#     # ë§ˆì§€ë§‰ ì¡°í•­ ì²˜ë¦¬ (ë¬¸ì„œ ëê¹Œì§€)
#     if current_article and buffer.strip():
#         st.write("Final article detected, saving...")
#         #st.write(f"Article: {current_article}, page_content: {buffer.strip()}")
#         docs.append(Document(
#             page_content=buffer.strip(),
#             metadata={
#                 "article": current_article,
#                 "start_page": start_page,
#                 "end_page": page_no  # ë§ˆì§€ë§‰ í˜ì´ì§€ê¹Œì§€
#             }
#         ))

#     return docs

def parse_articles(pages: List[Document]) -> List[Document]:
    docs = []
    buffer = ""
    current_article = None
    start_page = None
    last_content_page = None

    for page in pages:
        page_no = page.metadata.get("page", 0) + 1

        for line in page.page_content.splitlines():
            if ARTICLE_START_RE.match(line):
                if current_article:
                    docs.append(Document(
                        page_content=buffer.strip(),
                        metadata={
                            "article": current_article,
                            "start_page": start_page,
                            "end_page": last_content_page
                        }
                    ))
                current_article = line.strip()
                start_page = page_no
                buffer = line
                last_content_page = page_no
                continue

            if current_article:
                buffer += "\n" + line
                last_content_page = page_no  # â˜… í•µì‹¬

    if current_article and buffer.strip():
        docs.append(Document(
            page_content=buffer.strip(),
            metadata={
                "article": current_article,
                "start_page": start_page,
                "end_page": last_content_page
            }
        ))

    return docs

# =================================================
# ì¡°í•­ í…ìŠ¤íŠ¸ ì •ì œ
# =================================================
def clean_article_text(text: str) -> str:
    lines = text.splitlines()
    cleaned = []
    for line in lines:
        if re.match(r"ì œ\s*\d+í¸|ì œ\s*\d+ì¥|ì œ\s*\d+ì ˆ", line.strip()):
            break
        cleaned.append(line)
    return "\n".join(cleaned).strip()

# =================================================
# ì›ë¬¸ ë Œë”ë§ (í˜•ì‹ ìœ ì§€)
# =================================================
def render_original_text(text: str):
    for line in text.splitlines():
        line = line.rstrip()
        if re.match(r"[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨]", line.strip()):
            st.markdown(f"**{line}**")
        elif re.match(r"\d+\.|[ê°€-í•˜]\.", line.strip()):
            st.markdown(f"&nbsp;&nbsp;{line}", unsafe_allow_html=True)
        else:
            st.markdown(line)

# =================================================
# Chroma DB
# =================================================
def build_db(docs: List[Document], version: str) -> Chroma:
    persist_dir = f"./chroma_db/{version}"
    os.makedirs(persist_dir, exist_ok=True)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
    return Chroma.from_documents(
        documents=docs,
        embedding=embeddings,
        persist_directory=persist_dir,
        collection_name="insurance_terms",
    )

# =================================================
# ì‚¬ì´ë“œë°”
# =================================================
st.sidebar.header("âš™ ì„¤ì •")
VERSION = st.sidebar.selectbox("ì•½ê´€ ë²„ì „ ì„ íƒ", ["2025-01", "2024-01"])

# =================================================
# ë©”ì¸
# =================================================
uploaded_file = st.file_uploader("ğŸ“„ ìë™ì°¨ë³´í—˜ ì•½ê´€ PDF ì—…ë¡œë“œ", type=["pdf"])

if uploaded_file:
    pages = load_pdf(uploaded_file)
    toc_map = parse_toc(pages)
    docs = parse_articles(pages)

    st.write(f"### ì´ {len(docs)}ê°œì˜ ì¡°í•­ì´ íŒŒì‹±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    st.write(f"### docs: {docs}")

    for d in docs:
        d.page_content = clean_article_text(d.page_content)
        m = re.search(r"(ì œ\s*\d+ì¡°)", d.metadata["article"])
        if m:
            key = m.group(1).replace(" ", "")
            if key in toc_map:
                d.metadata["page"] = toc_map[key]

    db = build_db(docs, VERSION)

    article_map = {}
    for d in docs:
        m = re.search(r"ì œ\s*(\d+)\s*ì¡°", d.metadata["article"])
        if m:
            article_map[int(m.group(1))] = d.metadata["article"]

    articles = [article_map[n] for n in sorted(article_map.keys())]
    selected_article = st.sidebar.selectbox("ì¡°í•­ ì„ íƒ", articles)

    question = st.text_input("â“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

    if st.button("ì§ˆë¬¸í•˜ê¸°"):
        st.session_state.show_answer = True

    # =================================================
    # ì§ˆë¬¸ ê²°ê³¼
    # =================================================
    if st.session_state.show_answer and question:
        chat_box = st.empty()
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0,
            streaming=True,
            callbacks=[StreamHandler(chat_box)],
        )
        retriever = db.as_retriever(
            search_kwargs={"filter": {"article": selected_article}}
        )
        prompt = hub.pull("rlm/rag-prompt")
        rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        rag_chain.invoke(question)

    if st.button("ğŸ“„ ì„ íƒ ì¡°í•­ ì›ë¬¸ ë³´ê¸°"):
        st.session_state.show_original = True

    # =================================================
    # ì›ë¬¸ ë³´ê¸°
    # =================================================
    if st.session_state.show_original:
        st.divider()
        for d in docs:
            if d.metadata["article"] == selected_article:

                #st.write(d.metadata)

                start = d.metadata["start_page"]
                end = d.metadata["end_page"]
                article = d.metadata["article"]
                st.markdown(f"## {article} (p.{start}~p.{end})")

                # page = d.metadata["page"]
                # st.markdown(f"## {selected_article} (p.{page})")
                # st.link_button(
                #     "PDF í•´ë‹¹ í˜ì´ì§€ë¡œ ì´ë™",
                #     f"file:///{uploaded_file.name}#page={page}",
                # )

                #st.write(f"## d.page_content: {d.page_content}")

                render_original_text(d.page_content)
                break
