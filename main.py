import os
import re
import tempfile
from typing import List

import streamlit as st
from dotenv import load_dotenv

load_dotenv()

from langchain_community.document_loaders import PyPDFLoader
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.schema import Document
from langchain.callbacks.base import BaseCallbackHandler
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain import hub

# =================================================
# ê¸°ë³¸ UI
# =================================================
st.set_page_config(layout="wide")
st.title("ğŸ“˜ ìë™ì°¨ë³´í—˜ ì•½ê´€ RAG")

# =================================================
# ì„¸ì…˜ ìƒíƒœ (ë²„íŠ¼ í† ê¸€ ë°©ì§€)
# =================================================
if "show_answer" not in st.session_state:
    st.session_state.show_answer = False
if "show_original" not in st.session_state:
    st.session_state.show_original = False

# =================================================
# ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬
# =================================================
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.container = container
        self.text = ""

    def on_llm_new_token(self, token: str, **kwargs):
        self.text += token
        self.container.markdown(self.text)

# =================================================
# PDF ë¡œë“œ
# =================================================
def load_pdf(uploaded_file):
    tmp = tempfile.TemporaryDirectory()
    path = os.path.join(tmp.name, uploaded_file.name)
    with open(path, "wb") as f:
        f.write(uploaded_file.getvalue())
    return PyPDFLoader(path).load()

# =================================================
# ì •ê·œì‹
# =================================================
ARTICLE_START_RE = re.compile(r"(ì œ\s*\d+\s*ì¡°\s*\([^)]+\))")
STOP_RE = re.compile(r"(ì œ\s*\d+í¸|ì œ\s*\d+ì¥|ì œ\s*\d+ì ˆ)")

# =================================================
# ëª©ì°¨ íŒŒì‹± (í˜ì´ì§€ ì •í™•í™”)
# =================================================
def parse_toc(pages: List[Document]) -> dict:
    toc = {}
    for page in pages[:5]:
        for line in page.page_content.splitlines():
            m = re.search(r"(ì œ\s*\d+ì¡°).*?(\d+)$", line.strip())
            if m:
                toc[m.group(1).replace(" ", "")] = int(m.group(2))
    return toc

# =================================================
# ì¡°í•­ íŒŒì‹±
# =================================================
def parse_articles(pages: List[Document]) -> List[Document]:
    docs = []
    buffer = ""
    current_article = None
    current_page = None

    for page in pages:
        page_no = page.metadata.get("page", 0) + 1
        for line in page.page_content.splitlines():
            if ARTICLE_START_RE.match(line):
                if current_article:
                    docs.append(Document(
                        page_content=buffer.strip(),
                        metadata={"article": current_article, "page": current_page}
                    ))
                current_article = line.strip()
                current_page = page_no
                buffer = line
                continue

            if current_article and STOP_RE.match(line):
                docs.append(Document(
                    page_content=buffer.strip(),
                    metadata={"article": current_article, "page": current_page}
                ))
                current_article = None
                buffer = ""
                continue

            if current_article:
                buffer += "\n" + line

    if current_article and buffer.strip():
        docs.append(Document(
            page_content=buffer.strip(),
            metadata={"article": current_article, "page": current_page}
        ))

    return docs

# =================================================
# ì¡°í•­ í…ìŠ¤íŠ¸ ì •ì œ
# =================================================
def clean_article_text(text: str) -> str:
    lines = text.splitlines()
    cleaned = []
    for line in lines:
        if re.match(r"ì œ\s*\d+í¸|ì œ\s*\d+ì¥|ì œ\s*\d+ì ˆ", line.strip()):
            break
        cleaned.append(line)
    return "\n".join(cleaned).strip()

# =================================================
# ì›ë¬¸ ë Œë”ë§ (í˜•ì‹ ìœ ì§€)
# =================================================
def render_original_text(text: str):
    for line in text.splitlines():
        line = line.rstrip()
        if re.match(r"[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨]", line.strip()):
            st.markdown(f"**{line}**")
        elif re.match(r"\d+\.|[ê°€-í•˜]\.", line.strip()):
            st.markdown(f"&nbsp;&nbsp;{line}", unsafe_allow_html=True)
        else:
            st.markdown(line)

# =================================================
# Chroma DB
# =================================================
def build_db(docs: List[Document], version: str) -> Chroma:
    persist_dir = f"./chroma_db/{version}"
    os.makedirs(persist_dir, exist_ok=True)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
    return Chroma.from_documents(
        documents=docs,
        embedding=embeddings,
        persist_directory=persist_dir,
        collection_name="insurance_terms",
    )

# =================================================
# ì‚¬ì´ë“œë°”
# =================================================
st.sidebar.header("âš™ ì„¤ì •")
VERSION = st.sidebar.selectbox("ì•½ê´€ ë²„ì „ ì„ íƒ", ["2024-01", "2025-01"])

# =================================================
# ë©”ì¸
# =================================================
uploaded_file = st.file_uploader("ğŸ“„ ìë™ì°¨ë³´í—˜ ì•½ê´€ PDF ì—…ë¡œë“œ", type=["pdf"])

if uploaded_file:
    pages = load_pdf(uploaded_file)
    toc_map = parse_toc(pages)
    docs = parse_articles(pages)

    for d in docs:
        d.page_content = clean_article_text(d.page_content)
        m = re.search(r"(ì œ\s*\d+ì¡°)", d.metadata["article"])
        if m:
            key = m.group(1).replace(" ", "")
            if key in toc_map:
                d.metadata["page"] = toc_map[key]

    db = build_db(docs, VERSION)

    article_map = {}
    for d in docs:
        m = re.search(r"ì œ\s*(\d+)\s*ì¡°", d.metadata["article"])
        if m:
            article_map[int(m.group(1))] = d.metadata["article"]

    articles = [article_map[n] for n in sorted(article_map.keys())]
    selected_article = st.sidebar.selectbox("ì¡°í•­ ì„ íƒ", articles)

    question = st.text_input("â“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

    if st.button("ì§ˆë¬¸í•˜ê¸°"):
        st.session_state.show_answer = True

    if st.button("ğŸ“„ ì„ íƒ ì¡°í•­ ì›ë¬¸ ë³´ê¸°"):
        st.session_state.show_original = True

    # =================================================
    # ì§ˆë¬¸ ê²°ê³¼
    # =================================================
    if st.session_state.show_answer and question:
        chat_box = st.empty()
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0,
            streaming=True,
            callbacks=[StreamHandler(chat_box)],
        )
        retriever = db.as_retriever(
            search_kwargs={"filter": {"article": selected_article}}
        )
        prompt = hub.pull("rlm/rag-prompt")
        rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        rag_chain.invoke(question)

    # =================================================
    # ì›ë¬¸ ë³´ê¸°
    # =================================================
    if st.session_state.show_original:
        st.divider()
        for d in docs:
            if d.metadata["article"] == selected_article:
                page = d.metadata["page"]
                st.markdown(f"## {selected_article} (p.{page})")
                st.link_button(
                    "PDF í•´ë‹¹ í˜ì´ì§€ë¡œ ì´ë™",
                    f"file:///{uploaded_file.name}#page={page}",
                )
                render_original_text(d.page_content)
                break
